{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a01b636-727b-4ecb-9955-03911d9fd22a",
   "metadata": {},
   "source": [
    "# Exhibits from report *Beyond Actual to Table*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc5823-2bd0-41f4-a44d-95dcb0664b06",
   "metadata": {},
   "source": [
    "This notebook duplicates most of the exhibits in [Beyond Actual to Table](https://www.soa.org/resources/research-reports/2018/beyond-actual-table/).  Results differ slightly from the published report, due largely to slight differences between the published dataset used here and the earlier dataset available only to the ILEC.  I included most of the exhibits besides the tree model.\n",
    "\n",
    "Orignial SOA report appendix G:\n",
    "* Site: https://www.soa.org/resources/experience-studies/2017/2009-13-indiv-life-ins-mort-exp/\n",
    "* Appendices: https://www.soa.org/globalassets/assets/files/research/exp-study/2009-13-indiv-life-ins-mort-exp-appendix.xlsx\n",
    "\n",
    "Run the notebook to initialize the libraries and generate exhibits from the report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d609e1f-55b8-4db7-aeb5-27e7311d5a89",
   "metadata": {},
   "source": [
    "# Initialize libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6e50ee-2b0e-46b5-a236-c612e01faebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# set these paramaters for your environment before running.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m DIR_HOME \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mPath\u001b[49m\u001b[38;5;241m.\u001b[39mhome())\n\u001b[1;32m      3\u001b[0m DIR_DATA \u001b[38;5;241m=\u001b[39m DIR_HOME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/ilec/Data/Processed/2021_published\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# set to your directory\u001b[39;00m\n\u001b[1;32m      4\u001b[0m DIR_ILECTools \u001b[38;5;241m=\u001b[39m DIR_HOME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dev/ILECTools\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# set these paramaters for your environment before running.\n",
    "DIR_HOME = str(Path.home())\n",
    "DIR_DATA = DIR_HOME + '/data/ilec/Data/Processed/2021_published' # set to your directory\n",
    "DIR_ILECTools = DIR_HOME + '/dev/ILECTools' # if you have not installd the directory you'll need this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155007ac-352a-4f57-9578-c61539b7d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display  import display, HTML, Markdown\n",
    "\n",
    "# In case you haven't installed ILECTools, this bit will get the ilectools directory into \n",
    "# your search path to its functions can be imported.\n",
    "\n",
    "if not any([Path(_p).name=='ILECTools' for _p in sys.path]):\n",
    "    sys.path.append(DIR_ILECTools) # is where it is on this machine\n",
    "\n",
    "from ilectools import a2t\n",
    "\n",
    "%config Completer.use_jedi=False # helps with autocompleter in notebook in some configurations\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "pd.options.display.width=255\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470dbd8-e30e-455e-ace5-59301caa35d0",
   "metadata": {},
   "source": [
    "# Initialize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993178cf-37dc-4afd-aab1-96d537d669c4",
   "metadata": {},
   "source": [
    "The data must be preprocessed in advance.\n",
    "* Take subset for exhibit G.\n",
    "* Summarize data on variables used in models so Poisson regression will not choke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fd3ae-a516-40ee-8204-3c057f6a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = a2t.append_columns_for_model(\n",
    "    pd.read_parquet(DIR_DATA + '/ILEC 2009-18 20210528_v2.parquet')\n",
    "    )\n",
    "\n",
    "# subset of data for exhibit g, the example used in \n",
    "data_exhibit_g = a2t.exhibit_g_subset(data)\n",
    "\n",
    "del data # why not free up a few gb of RAM\n",
    "# tidiness\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee0d3f-f13d-4528-81d7-b1358b709fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model will run better on a summary - or: will run at all\n",
    "# Fields aggregated here, value columns    \n",
    "data_exhibit_g_summary = data_exhibit_g.pivot_table(\n",
    "    index='uw,face_amount_band,observation_year,dur_band1,ia_band1,gender,insurance_plan,ltp,iy_band1'.split(','), \n",
    "    values=['policy_actual', 'policy_2015vbt', 'amount_actual', 'amount_2015vbt'], \n",
    "    aggfunc=np.sum).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8b632-5b28-441f-8e45-dbaa3019bffe",
   "metadata": {},
   "source": [
    "# Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b6484-22f9-4306-9b6f-4c3c568185f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get policy and amount models\n",
    "models = {\n",
    "    m:a2t.PoissonWrapper(\n",
    "        offset_column=f'{m}_2015vbt'\n",
    "      , formula = f\"\"\"{m}_actual ~\n",
    "                + C(uw, Treatment(reference='N/2/1'))\n",
    "                + C(dur_band1, Treatment(reference='06-10'))\n",
    "                + C(face_amount_band, Treatment(reference='  100000-249999'))\n",
    "                + C(ia_band1, Treatment(reference='40-44'))\n",
    "                + C(gender)\n",
    "                + C(observation_year)\n",
    "                + C(insurance_plan, Treatment(reference='Term'))\n",
    "                + C(ltp, Treatment(reference='20 yr or N/A (Not Term)'))\n",
    "                + C(iy_band1)\n",
    "            \"\"\"\n",
    "      , data = data_exhibit_g_summary\n",
    "    )\n",
    "    for m\n",
    "    in ['policy', 'amount']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f23e6-1060-4685-a391-d009b93dde79",
   "metadata": {},
   "source": [
    "Append model results to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2108b97-b78d-4e8c-b30d-3e0c7ed8f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_item in ['policy', 'amount']:\n",
    "    data_exhibit_g_summary[f'{model_item}_{model_item}model'] = models[model_item].fit.predict()\n",
    "# adjust each 2015vbt tabular amount (policy or amount) by the model factor from modelling the other one:\n",
    "data_exhibit_g_summary['policy_amountmodel'] = (data_exhibit_g_summary['amount_amountmodel'] / data_exhibit_g_summary['amount_2015vbt']\n",
    "                                                * data_exhibit_g_summary['policy_2015vbt'])\n",
    "data_exhibit_g_summary['amount_policymodel'] = (data_exhibit_g_summary['policy_policymodel'] / data_exhibit_g_summary['policy_2015vbt']\n",
    "                                                * data_exhibit_g_summary['amount_2015vbt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efe989-383a-4a6b-8ec4-5057dc36f3d6",
   "metadata": {},
   "source": [
    "Using factors for the amount model will duplicate the total amount, using factors for the policy model will return a bit different amount in total, and vice versa for the policy count model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d6c59-f1a3-4ba2-8bbe-1c6ac6c7e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exhibit_g_summary.filter(regex='(model|actual)$').sum().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd800ce-842b-486e-be9f-79869880cae9",
   "metadata": {},
   "source": [
    "# Appendix 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30bb70-b644-4d05-a0fe-d6b80de47233",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t0, basis in zip([2, 3, 4],\n",
    "                     ['2015vbt', 'policymodel', 'amountmodel']):\n",
    "    for t1, metric in zip(['a', 'b'], \n",
    "                          ['policy', 'amount']):\n",
    "        caption = f\"Table {t0}{t1}: Exhibit G actual / {basis} by {metric}; by Plan and Duration\"\n",
    "        display(Markdown('## '+caption)) # show here instead of html table caption to use the TOC feature of notebooks\n",
    "        display(a2t.get_exhibit_g(data_exhibit_g_summary, metric, basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc187e67-ebc2-4bd6-ad30-fe30f507f40a",
   "metadata": {},
   "source": [
    "# Appendix 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97effe59-7f3e-4419-bbae-9640a66c8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from IPython.display import Markdown\n",
    "combos = list(itertools.product(\n",
    "    ['policy', 'amount'], \n",
    "    ['iy_band1', 'uw', 'insurance_plan', 'ltp', 'ia_band1', 'dur_band1', 'observation_year', 'gender', 'face_amount_band']\n",
    "        ))\n",
    "plots = {k:m.plot_comparison() for k, m in models.items()} # the plot_comparison precomputes the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87ea09-b30c-456d-9291-79994035a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (metric, split) in enumerate(combos):\n",
    "    # caption\n",
    "    display(Markdown(f'## Figure {i+4}: Model by {metric}, factors by {split} vs Actual / 2015VBT'))\n",
    "    m = models[metric]\n",
    "    \n",
    "    # graph\n",
    "    display(plots[metric][split])\n",
    "    \n",
    "    # table of numbers\n",
    "    display(m.factor_analysis_exhibit(split))\n",
    "    \n",
    "    display(HTML('<p/>')) # for some space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42388534-8186-4076-93a2-04e9d3c0d3a4",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilec0]",
   "language": "python",
   "name": "conda-env-ilec0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
